get_leaves()
get_phrase_type_regex(x, "VP") %>%
take() %>%
get_phrase_type_regex("NP") %>%
take() %>%
get_leaves()
get_phrase_type(x, "NP")
res = get_phrase_type(x, "NP")
class(res)
len(res)
length(res)
res[[1]]
get_leaves(res[[1]])
get_leaves((res[[1]]))
get_leaves((res[[2]]))
get_leaves((res[[3]]))
x
txt
get_leaves((res[[4]]))
get_leaves((res[[5]]))
get_leaves((res[[6]]))
get_leaves((res[[7]]))
x = matrix(sample(0:9, 20, replace=TRUE), nrow=5)
x
colSums(x)
library(goodies)
library(quantmod)
library(goodies)
library(quantmod)
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/Monumental_Day"
setwd(home)
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/Monumental_Day"
setwd(home)
library(goodies)
library(quantmod)
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/Monumental_Day"
setwd(home)
library(quantmod)
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance"
utils = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance/utils"
plot_dir = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance/plots"
dat_dir = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_jun27"
setwd(home)
source(paste(utils, "/Fun.R", sep=""))
# Get all CSV file names
nm = list.files(path=dat_dir, all.files=FALSE, include.dirs=FALSE)
head(nm)
?list.files
R.home("doc")
list.files(R.home("doc"))
setwd(home)
# Get all CSV file names
nm = list.files(path=dat_dir, all.files=FALSE, include.dirs=FALSE)
nm = nm[3:length(nm)]
head(nm, 20)
(N = length(nm))
# 8482
# Build vector of ticker symbols
tkr = sapply(strsplit(x=nm, split="\\."), parse_ticker)
head(tkr, 20)
# Check for issues
sum(is.na(tkr)) # expect 0
# Read CSV files, convert them to xts objects, save them to the data_env environment
setwd(dat_dir)
data_env = new.env()
res = sapply(nm, read_convert_save2env, data_env=data_env)
# Check : # tickers match # of time series
length(ls(envir=data_env)) == length(nm) # expect TRUE
# Check : time series lengths
X11();hist(res)
# Data issue : some time series are much longer than others. Duplicate entries.
which(res > 30000)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  19   15136   17828   15097   17861   32077
# time series lengths
setwd(home)
png("LengthDistrib.png", width=640, height=480)
hist(Len, breaks=35, xlab="No of observations in a CSV file", main="Distribution of Time Series Lengths")
dev.off()
Len = res
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  19   15136   17828   15097   17861   32077
# time series lengths
setwd(home)
png("LengthDistrib.png", width=640, height=480)
hist(Len, breaks=35, xlab="No of observations in a CSV file", main="Distribution of Time Series Lengths")
dev.off()
# Data issue : some time series are much longer than others. Duplicate entries.
which(Len > 25000)
# Look at their time steps =================================================
ll = sweep_env(data_env, xts_time_spacing)
# Over all the tickers, what percent of the time intervals falls in the half-hr to 1 hr bucket?
pct_1hr = sapply(ll, function(.x) .x[["granu"]][["1hr"]] / sum(.x[["granu"]]))
png("OneHrIntervalDistrib.png", width=640, height=480)
hist(pct_1hr, breaks=30, xlab="Fraction of 1-hr Intervals", main="Distribution of One-Hour Intervals")
dev.off()
# Over all the tickers, what percent of the time intervals falls in the 0 to 5-min bucket?
pct_5min = sapply(ll, function(.x) .x[["granu"]][["5min"]] / sum(.x[["granu"]]))
png("5minIntervalDistrib.png", width=640, height=480)
hist(pct_5min, breaks=30, xlab="Fraction of 5-min Intervals", main="Distribution of 5-min Intervals")
dev.off()
# Observation : very few. But there are some tickers with more than 40% deltas in the 5min bucket.
# That smells trouble. Who are these ?
sapply(which(pct_5min > 0.2), function(.i)ll[[.i]][["tkr"]]) # more than 20% of intervals are in this 5-min bucket
# How many tickers have near zero holding over the entire time period (2 yrs)?
ll1 = sweep_env(data_env, holding_below_threshold, thres=1e-2)
ix.ave = sapply(ll1, function(.x).x[["ave.below"]])
table(ix.ave)
ix.med = sapply(ll1, function(.x).x[["med.below"]])
table(ix.med)
ix.latest = sapply(ll1, function(.x).x[["latest.n.below"]])
table(ix.latest)
sapply(which(ix.ave), function(.i)ll1[[.i]][["tkr"]])
len(sapply(which(ix.med), function(.i)ll1[[.i]][["tkr"]]))
library(goodies)
len(sapply(which(ix.med), function(.i)ll1[[.i]][["tkr"]]))
sapply(which(ix.latest), function(.i)ll1[[.i]][["tkr"]])
setwd(home)
# Aggregate the hourly data to daily and weekly and put them in a list
ll = sweep_env(data_env, day_wk_aggregator)
# Check + statistics on each ticker dataset
cnt = table(sapply(ll, function(.s) nrow(.s[["y_daily"]])))
sum(cnt[which(as.integer(names(cnt)) > 365)]) / len(tkr)
# [1] 0.8450455
# ==> 84% of the tickers have more than 1 yr of data
table(sapply(ll, function(.s) nrow(.s[["dy_daily"]])))
cnt = table(sapply(ll, function(.s) nrow(.s[["y_wk"]])))
sum(cnt[which(as.integer(names(cnt)) > 52)]) / len(tkr)
# [1] 0.8488
# ==> 84% of the tickers have more than 52 wks of data
table(sapply(ll, function(.s) nrow(.s[["dy_wk"]])))
# 2. Top holdings
df = as.data.frame(t(sapply(ll, latest_stat, "2020-03-20")))
df[,"tkr"] = as.character(df[,"tkr"])
df[,"base_holding"] = as.numeric(df[,"base_holding"])
df[,"cur_holding"] = as.numeric(df[,"cur_holding"])
df[,"pct_change"] = 100*(df[,"cur_holding"] / df[,"base_holding"] - 1)
df = df[order(df[,"cur_holding"], decreasing = TRUE),]
top_nm = head(df, 10)[,"tkr"]
top_nm
# 3. stocks with more than 100k accounts
df[which(df[,"cur_holding"] > 100000), "tkr"]
# 4. largest percentage increase since Mar 20
df1 = head(df, 100)
df1 = df1[order(df1[,"pct_change"], decreasing=TRUE),]
incr_nm = head(df1, 10)[,"tkr"]
head(df1, 10)
library(gridExtra)
df = data.frame("variables" = c("d_agr","d_def","d_frig","d_hidro","d_roads","d_silos"),
"coeficient" = c(0.18,0.19,-0.01,-0.25,-0.17,0.09))
png("output.png", width=480,height=480,bg = "white")
grid.table(df)
dev.off()
df
rownames(df) <- ""
rownames(df) <- rep("", 6)
df
rownames(df) <- NULL
df
png("output.png", width=480,height=480,bg = "white")
grid.table(df)
dev.off()
?grid.table
png("output.png", width=480,height=480,bg = "white")
grid.table(df, row.names=FALSE)
dev.off()
png("output.png", width=480,height=480,bg = "white")
grid.table(df, rows=NULL)
dev.off()
head(df1, 10)
# for github display
tmp = head(df1, 10)
colnames(tmp) = c("Ticker", "Mar 20th", "This Week", "% Change")
tmp
tmp[,2] = format(tmp[,2],big.mark=",",scientific=FALSE)
tmp
tmp[,3] = format(tmp[,3],big.mark=",",scientific=FALSE)
tmp
?format
tmp[,4] = format(tmp[,4],big.mark=",", digits=2, scientific=FALSE)
tmp
sapply(tmp, class)
# for github display
tmp = head(df1, 10)
colnames(tmp) = c("Ticker", "Mar 20th", "This Week", "% Change")
tmp[,2] = format(tmp[,2], big.mark=",", digits=0, scientific=FALSE)
tmp[,3] = format(tmp[,3], big.mark=",", digits=0, scientific=FALSE)
tmp[,4] = format(tmp[,4], big.mark=",", digits=2, scientific=FALSE)
tmp
tmp = head(df, 10)
tmp
# 2. Top holdings
df = as.data.frame(t(sapply(ll, latest_stat, "2020-03-20")))
df[,"tkr"] = as.character(df[,"tkr"])
df[,"base_holding"] = as.numeric(df[,"base_holding"])
df[,"cur_holding"] = as.numeric(df[,"cur_holding"])
df[,"pct_change"] = 100*(df[,"cur_holding"] / df[,"base_holding"] - 1)
df = df[order(df[,"cur_holding"], decreasing = TRUE),]
top_nm = head(df, 10)[,"tkr"]
tmp = head(df, 10)
tmp
colnames(tmp) = c("Ticker", "Mar 20th", "This Week", "% Change")
tmp[,2] = format(tmp[,2], big.mark=",", digits=0, scientific=FALSE)
tmp[,3] = format(tmp[,3], big.mark=",", digits=0, scientific=FALSE)
tmp[,4] = format(tmp[,4], big.mark=",", digits=2, scientific=FALSE)
tmp
png("LargestIncr.png", width=480, height = 480, bg="white")
grid.table(tmp)
dev.off()
?grid.table
png("LargestIncr.png", width=480, height = 480, bg="white")
grid.table(tmp)
dev.off()
png("Top10.png", width=480, height = 480, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
tmp = head(df, 10)
colnames(tmp) = c("Ticker", "Mar 20th", "This Week", "% Change")
tmp[,2] = format(tmp[,2], big.mark=",", digits=0, scientific=FALSE)
tmp[,3] = format(tmp[,3], big.mark=",", digits=0, scientific=FALSE)
tmp[,4] = format(tmp[,4], big.mark=",", digits=2, scientific=FALSE)
png("Top10_table.png", width=1080, height = 1080, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=2000, height =2000, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=100, height =100, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=200, height =100, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=400, height =400, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=300, height =300, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=300, height =300, bg="white")
grid.table(tmp, rows=NULL, padding=unit(c(8,4)))
dev.off()
png("Top10_table.png", width=300, height =300, bg="white")
grid.table(tmp, rows=NULL, padding=c(8,4))
dev.off()
png("Top10_table.png", width=300, height =300, bg="white")
grid.table(tmp, rows=NULL, padding=unit(c(8,4), "mm"))
dev.off()
png("Top10_table.png", width=640, height=640, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=480, height=480, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
png("Top10_table.png", width=300, height=300, bg="white")
grid.table(tmp, rows=NULL)
dev.off()
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/AlgoTrading/Robinhood"
setwd(home)
home = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood"
setwd(home)
load("Robintrack.RData")
len(tkr)
len(ll)
ls(envir=data_env)
len(ls(envir=data_env))
dim(df)
dim(df1)
sapply(top_nm, function(.k) getSymbols(.k, env=globalenv(), src="yahoo", from="1800-01-01"))
tail(AAPL)
tail(CCL)
ll.old = ll
len(ll.old)
# NOTE : call fun.R *after* reading in the old data, never before !
source(paste(utils, "/Fun.R", sep=""))
# Get all CSV file names
nm = list.files(path=dat_dir, all.files=FALSE, include.dirs=FALSE)
nm = nm[3:length(nm)]
(N = length(nm))
dat_dir = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_jun27"
nm
dat_dir = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_jun27"
home = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance"
home1 = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood"
utils = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance/utils"
plot_dir = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance/plots"
# dat_dir = "/mnt/WanChai/Dropbox/GITHUB_REPO/Robinhood_at_a_glance/robintrack_popularity_export"
dat_cur = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_CUR"
dat_last = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_LASTWK"
# Get last wk's data for comparison
setwd(home1)
load("Robintrack.RData")
ll.old = ll
len(ll)
# NOTE : call fun.R *after* reading in the old data, never before !
source(paste(utils, "/Fun.R", sep=""))
# Get all CSV file names
setwd(home)
nm = list.files(path=dat_cur, all.files=FALSE, include.dirs=FALSE)
nm = nm[3:length(nm)]
(N = length(nm))
# 8482
# Build vector of ticker symbols
tkr = sapply(strsplit(x=nm, split="\\."), parse_ticker)
# Check for issues
sum(is.na(tkr)) # expect 0
rm(data_env)
# Read CSV files, convert them to xts objects, save them to the data_env environment
setwd(dat_cur)
data_env = new.env()
Len = sapply(nm, read_convert_save2env, data_env=data_env)
# Check : # tickers match # of time series
length(ls(envir=data_env)) == length(nm) # expect TRUE
len(Len)
setwd(home)
# Aggregate the hourly data to daily and weekly and put them in a list
ll = sweep_env(data_env, day_wk_aggregator)
# Check + statistics on each ticker dataset
cnt = table(sapply(ll, function(.s) nrow(.s[["y_daily"]])))
sum(cnt[which(as.integer(names(cnt)) > 365)]) / len(tkr)
# [1] 0.8450455
# ==> 84% of the tickers have more than 1 yr of data
table(sapply(ll, function(.s) nrow(.s[["dy_daily"]])))
cnt = table(sapply(ll, function(.s) nrow(.s[["y_wk"]])))
sum(cnt[which(as.integer(names(cnt)) > 52)]) / len(tkr)
# [1] 0.8488
# ==> 84% of the tickers have more than 52 wks of data
table(sapply(ll, function(.s) nrow(.s[["dy_wk"]])))
# 2. Top holdings
df = as.data.frame(t(sapply(ll, latest_stat, "2020-03-20")))
df[,"tkr"] = as.character(df[,"tkr"])
df[,"base_holding"] = as.numeric(df[,"base_holding"])
df[,"cur_holding"] = as.numeric(df[,"cur_holding"])
df[,"pct_change"] = 100*(df[,"cur_holding"] / df[,"base_holding"] - 1)
df = df[order(df[,"cur_holding"], decreasing = TRUE),]
top_nm = head(df, 10)[,"tkr"]
tmp = head(df, 10)
tmp
dim(df)
dim(df.old)
# 7. biggest change since last week
df.old = df = as.data.frame(t(sapply(ll.old, latest_stat, "2020-03-20")))
df.old[,"tkr"] = as.character(df.old[,"tkr"])
df.old[,"base_holding"] = as.numeric(df.old[,"base_holding"])
df.old[,"cur_holding"] = as.numeric(df.old[,"cur_holding"])
df.old[,"pct_change"] = 100*(df.old[,"cur_holding"] / df.old[,"base_holding"] - 1)
df.old = df.old[order(df.old[,"cur_holding"], decreasing = TRUE),]
dim(df.old)
head(df.old)
?merge
df.comb = merge(df, df.old, by="tkr", all.x=FALSE, all.y=TRUE)
class(df)
class(df.old)
head(df)
dim(df)
len(ll)
sapply(df, class)
df[,"tkr"] = as.character(df[,"tkr"])
df[,"base_holding"] = as.numeric(df[,"base_holding"])
df[,"cur_holding"] = as.numeric(df[,"cur_holding"])
df[,"pct_change"] = 100*(df[,"cur_holding"] / df[,"base_holding"] - 1)
df = df[order(df[,"cur_holding"], decreasing = TRUE),]
sapply(df, class)
head(df)
df.comb = merge(df, df.old, by="tkr", all.x=FALSE, all.y=TRUE)
head(df.comb)
df.comb = merge(df, df.old, by="tkr", all.x=FALSE, all.y=TRUE, suffixes=c(".cur", ".lastwk"))
head(df.comb)
df.comb = df.comb[order(df.comb[,"cur_holding.cur"], decreasing=TRUE),]
head(df.comb)
df.comb = merge(df[,1:3], df.old[,1:3], by="tkr", all.x=FALSE, all.y=TRUE, suffixes=c(".cur", ".lastwk"))
df.comb = df.comb[order(df.comb[,"cur_holding.cur"], decreasing=TRUE),]
head(df.comb)
del = df.comb[,3]-df.comb[,5]
summary(del)
# Get all CSV file names
setwd(home)
nm = list.files(path=dat_cur, all.files=FALSE, include.dirs=FALSE)
nm = nm[3:length(nm)]
(N = length(nm))
# 8482
# Build vector of ticker symbols
tkr = sapply(strsplit(x=nm, split="\\."), parse_ticker)
# Check for issues
sum(is.na(tkr)) # expect 0
rm(data_env)
# Read CSV files, convert them to xts objects, save them to the data_env environment
setwd(dat_cur)
data_env = new.env()
Len = sapply(nm, read_convert_save2env, data_env=data_env)
len(Len)
# Check : # tickers match # of time series
length(ls(envir=data_env)) == length(nm) # expect TRUE
setwd(home)
# Aggregate the hourly data to daily and weekly and put them in a list
ll = sweep_env(data_env, day_wk_aggregator)
# 2. Top holdings
df = as.data.frame(t(sapply(ll, latest_stat, "2020-03-20")))
df[,"tkr"] = as.character(df[,"tkr"])
df[,"base_holding"] = as.numeric(df[,"base_holding"])
df[,"cur_holding"] = as.numeric(df[,"cur_holding"])
df[,"pct_change"] = 100*(df[,"cur_holding"] / df[,"base_holding"] - 1)
df = df[order(df[,"cur_holding"], decreasing = TRUE),]
head(df)
head(df.old)
# 7. biggest change since last week
df.old = as.data.frame(t(sapply(ll.old, latest_stat, "2020-03-20")))
df.old[,"tkr"] = as.character(df.old[,"tkr"])
df.old[,"base_holding"] = as.numeric(df.old[,"base_holding"])
df.old[,"cur_holding"] = as.numeric(df.old[,"cur_holding"])
df.old[,"pct_change"] = 100*(df.old[,"cur_holding"] / df.old[,"base_holding"] - 1)
df.old = df.old[order(df.old[,"cur_holding"], decreasing = TRUE),]
head(df.old)
# 7. biggest change since last week
df.old = as.data.frame(t(sapply(ll.old, latest_stat, "2020-03-20")))
df.old[,"tkr"] = as.character(df.old[,"tkr"])
df.old[,"base_holding"] = as.numeric(df.old[,"base_holding"])
df.old[,"cur_holding"] = as.numeric(df.old[,"cur_holding"])
df.old = df.old[order(df.old[,"cur_holding"], decreasing = TRUE),]
head(df.old)
df.comb = merge(df[,1:3], df.old[,1:3], by="tkr", all.x=FALSE, all.y=TRUE, suffixes=c(".cur", ".lastwk"))
head(df.comb)
df.comb = df.comb[order(df.comb[,"cur_holding.cur"], decreasing=TRUE),]
head(df.comb)
df.comb[,"delta"] = df.comb[,"cur_holding.cur"] - df.comb[,"cur_holding.lastwk"]
df.comb = df.comb[order(df.comb[,"delta"], decreasing=TRUE),]
head(df.comb, 20)
df.comb[,"delta.pct"] = ifelse( df.comb[,"cur_holding.lastwk"] > 0, df.comb[,"delta"] / df.comb[,"cur_holding.lastwk"], NA)
df.comb = df.comb[order(df.comb[,"delta.pct"], decreasing=TRUE),]
head(df.comb)
head(df.comb, 10)
ix = which(df.comb[,"cur_holding.lastwk"] > 100000)
len(ix)
tmp = df.comb[ix,]
tmp
df.comb = merge(df[,1:3], df.old[,1:3], by="tkr", all.x=FALSE, all.y=TRUE, suffixes=c(".cur", ".lastwk"))
df.comb[,"delta"] = df.comb[,"cur_holding.cur"] - df.comb[,"cur_holding.lastwk"]
df.comb[,"delta.pct"] = ifelse( df.comb[,"cur_holding.lastwk"] > 0, df.comb[,"delta"] / df.comb[,"cur_holding.lastwk"], NA)
tmp = df.comb[which(df.comb[,"cur_holding.lastwk"] > 100000),]
tmp = tmp[order(tmp[,"delta.pct"], decreasing=TRUE),]
head(tmp, 10)
df.comb = merge(df[,1:3], df.old[,1:3], by="tkr", all.x=FALSE, all.y=TRUE, suffixes=c(".cur", ".lastwk"))
df.comb[,"delta"] = df.comb[,"cur_holding.cur"] - df.comb[,"cur_holding.lastwk"]
df.comb[,"delta.pct"] = ifelse( df.comb[,"cur_holding.lastwk"] > 0, 100*df.comb[,"delta"] / df.comb[,"cur_holding.lastwk"], NA)
tmp = df.comb[which(df.comb[,"cur_holding.lastwk"] > 100000),]
tmp = tmp[order(tmp[,"delta.pct"], decreasing=TRUE),]
head(tmp)
head(tmp, 10)
tmp = tmp[,c("tkr", "cur_holding.cur", "cur_holding.lastwk", "delta", "delta.pct")]
head(tmp, 10)
tail(tmp, 10)
incr = head(tmp, 10)
decr = tail(tmp, 10)
colnames(incr) = c("Ticker", "Current", "Last Week", "Net Change", "% Change")
incr[,2] = format(incr[,2], big.mark=",", digits=0, scientific=FALSE)
incr[,3] = format(incr[,3], big.mark=",", digits=0, scientific=FALSE)
incr[,4] = format(incr[,4], big.mark=",", digits=0, scientific=FALSE)
incr[,5] = format(incr[,4], big.mark=",", digits=2, scientific=FALSE)
incr
head(tmp)
incr = head(tmp, 10)
decr = tail(tmp, 10)
colnames(incr) = c("Ticker", "Current", "Last Week", "Net Change", "% Change")
incr[,2] = format(incr[,2], big.mark=",", digits=0, scientific=FALSE)
incr[,3] = format(incr[,3], big.mark=",", digits=0, scientific=FALSE)
incr[,4] = format(incr[,4], big.mark=",", digits=0, scientific=FALSE)
incr[,5] = format(incr[,5], big.mark=",", digits=2, scientific=FALSE)
incr
incr = head(tmp, 10)
decr = tail(tmp, 10)
colnames(incr) = c("Ticker", "Current", "Last Week", "Net Change", "% Change")
incr[,2] = format(incr[,2], big.mark=",", digits=0, scientific=FALSE)
incr[,3] = format(incr[,3], big.mark=",", digits=0, scientific=FALSE)
incr[,4] = format(incr[,4], big.mark=",", digits=0, scientific=FALSE)
incr[,5] = format(incr[,5], big.mark=",", digits=3, scientific=FALSE)
colnames(decr) = c("Ticker", "Current", "Last Week", "Net Change", "% Change")
decr[,2] = format(decr[,2], big.mark=",", digits=0, scientific=FALSE)
decr[,3] = format(decr[,3], big.mark=",", digits=0, scientific=FALSE)
decr[,4] = format(decr[,4], big.mark=",", digits=0, scientific=FALSE)
decr[,5] = format(decr[,5], big.mark=",", digits=3, scientific=FALSE)
incr
png("Incr_since_lastwk.png", width=480, height=480, bg="white")
grid.table(incr)
dev.off()
decr
png("Decr_since_lastwk.png", width=480, height = 480, bg="white")
grid.table(decr)
dev.off()
png("Incr_since_lastwk.png", width=480, height=480, bg="white")
grid.table(incr, rows=NULL)
dev.off()
png("Decr_since_lastwk.png", width=480, height = 480, bg="white")
grid.table(decr, rows=NULL)
dev.off()
png("Incr_since_lastwk.png", width=300, height=300, bg="white")
grid.table(incr, rows=NULL)
dev.off()
png("Decr_since_lastwk.png", width=300, height=300, bg="white")
grid.table(decr, rows=NULL)
dev.off()
png("Incr_since_lastwk.png", width=300, height=300, bg="white")
grid.table(incr, rows=NULL)
dev.off()
png("Decr_since_lastwk.png", width=300, height=300, bg="white")
grid.table(decr, rows=NULL)
dev.off()
png("Incr_since_lastwk.png", width=400, height=300, bg="white")
grid.table(incr, rows=NULL)
dev.off()
png("Decr_since_lastwk.png", width=400, height=300, bg="white")
grid.table(decr, rows=NULL)
dev.off()
