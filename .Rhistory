getwd()
library(quantmod)
library(gridExtra)
setwd("../")
home = getwd()
home1 = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood"
utils = paste(home, "/utils", sep="")
plot_dir = paste(home, "/plots", sep="")
dat_cur = paste(home, "/popularity_export", sep="")
dat_cur1 = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export"
dat_lastwk = "/mnt/WanChai/Dropbox/AlgoTrading/Robinhood/popularity_export_LASTWK"
# NOTE : call fun.R *after* reading in the old data, never before !
source(paste(utils, "/Fun.R", sep=""))
# Get all CSV file names
nm = list.files(path=dat_lastwk, all.files=FALSE, include.dirs=FALSE)
length(nm)
head(nm)
nm = nm[3:length(nm)]
(N = length(nm))
# 8508
# Build vector of ticker symbols
tkr = sapply(strsplit(x=nm, split="\\."), parse_ticker)
# Check for issues
sum(is.na(tkr)) # expect 0
# Read CSV files, convert them to xts objects, save them to the data_env environment
setwd(dat_lastwk)
data_env = new.env()
Len = sapply(nm, read_convert_save2env, data_env=data_env)
# Check : # tickers match # of time series
length(ls(envir=data_env)) == length(nm) # expect TRUE
getwd()
setwd(home)
# Aggregate the hourly data to daily and weekly and put them in a list
ll = sweep_env(data_env, day_wk_aggregator)
# Check + statistics on each ticker dataset
cnt = table(sapply(ll, function(.s) nrow(.s[["y_daily"]])))
sum(cnt[which(as.integer(names(cnt)) > 365)]) / len(tkr)
# [1] 0.8450455
# ==> 84% of the tickers have more than 1 yr of data
table(sapply(ll, function(.s) nrow(.s[["dy_daily"]])))
cnt = table(sapply(ll, function(.s) nrow(.s[["y_wk"]])))
sum(cnt[which(as.integer(names(cnt)) > 52)]) / len(tkr)
# [1] 0.8488
# ==> 84% of the tickers have more than 52 wks of data
table(sapply(ll, function(.s) nrow(.s[["dy_wk"]])))
ls()
ls(envir=data_env)
len(ls(envir=data_env))
length(ls(envir=data_env))
getwd()
save.image("Robintrack.RData")
